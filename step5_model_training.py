import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, recall_score, f1_score, roc_auc_score
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

print("===== Step 5ï¼šè¿çº¦é¢„æµ‹æ¨¡å‹è®­ç»ƒ =====")

# 1. è¯»å–æœ€ç»ˆæ•°æ®é›†
df = pd.read_csv("model_dataset.csv")

# -------------------------------------------------------------
# 2. é€‰æ‹©ç”¨äºå»ºæ¨¡çš„ç‰¹å¾ï¼ˆé£æ§ä¸­å¸¸ç”¨çš„å…³é”®ç‰¹å¾ï¼‰
feature_cols = [
    "age", "max_open_months", "total_credit_limit",
    "recent_3m_amount", "credit_usage_rate",
    "recent_3m_overdue_cnt", "max_overdue_days",
    "avg_overdue_days"
]

X = df[feature_cols]
y = df["label_default"]  # ç›®æ ‡å˜é‡ï¼ˆæ˜¯å¦è¿çº¦ï¼‰

# ğŸ”¥ å…³é”®æ­¥éª¤ï¼šæŠŠç‰¹å¾é‡Œçš„ NaN ç»Ÿä¸€å¡«æˆ 0ï¼Œé¿å…æ¨¡å‹æŠ¥é”™
X = X.fillna(0)

# å¯é€‰ï¼šæ‰“å°ä¸€ä¸‹æ¯ä¸€åˆ—è¿˜æœ‰æ²¡æœ‰ NaNï¼ˆæ­£å¸¸æƒ…å†µåº”è¯¥éƒ½æ˜¯ 0ï¼‰
print("æ¯ä¸ªç‰¹å¾é‡Œ NaN çš„æ•°é‡ï¼š")
print(X.isna().sum())


# -------------------------------------------------------------
# 3. åˆ’åˆ†è®­ç»ƒé›† / æµ‹è¯•é›†
# -------------------------------------------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=42
)

# -------------------------------------------------------------
# 4. å»ºç«‹é€»è¾‘å›å½’æ¨¡å‹ï¼ˆé£æ§æœ€å¸¸ç”¨ï¼‰
# -------------------------------------------------------------
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

logit = LogisticRegression(max_iter=500)
logit.fit(X_train_scaled, y_train)

y_pred_logit = logit.predict(X_test_scaled)
y_prob_logit = logit.predict_proba(X_test_scaled)[:, 1]

# -------------------------------------------------------------
# 5. å»ºç«‹éšæœºæ£®æ—æ¨¡å‹ï¼ˆè¡¨ç°æ›´å¥½ï¼‰
# -------------------------------------------------------------
rf = RandomForestClassifier(
    n_estimators=300,
    max_depth=6,
    random_state=42
)
rf.fit(X_train, y_train)

y_pred_rf = rf.predict(X_test)
y_prob_rf = rf.predict_proba(X_test)[:, 1]

# -------------------------------------------------------------
# 6. è¾“å‡ºæ¨¡å‹è¯„ä¼°ç»“æœï¼ˆé£æ§æœ€å…³æ³¨ Recall/AUCï¼‰
# -------------------------------------------------------------
print("\n===== é€»è¾‘å›å½’æ¨¡å‹è¡¨ç° =====")
print("Accuracy:", accuracy_score(y_test, y_pred_logit))
print("Recall:", recall_score(y_test, y_pred_logit))
print("F1 Score:", f1_score(y_test, y_pred_logit))
print("AUC:", roc_auc_score(y_test, y_prob_logit))

print("\n===== éšæœºæ£®æ—æ¨¡å‹è¡¨ç° =====")
print("Accuracy:", accuracy_score(y_test, y_pred_rf))
print("Recall:", recall_score(y_test, y_pred_rf))
print("F1 Score:", f1_score(y_test, y_pred_rf))
print("AUC:", roc_auc_score(y_test, y_prob_rf))

# -------------------------------------------------------------
# 7. æ‰“å°éšæœºæ£®æ—æ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§
# -------------------------------------------------------------
importance = pd.DataFrame({
    "feature": feature_cols,
    "importance": rf.feature_importances_
}).sort_values(by="importance", ascending=False)

print("\n===== éšæœºæ£®æ—ç‰¹å¾é‡è¦æ€§ï¼ˆä»é«˜åˆ°ä½ï¼‰=====")
print(importance)

# ä¿å­˜ç‰¹å¾é‡è¦æ€§åˆ°æ–‡ä»¶
importance.to_csv("model_feature_importance.csv", index=False, encoding="utf-8-sig")

print("\nğŸ‰ Step 5 å®Œæˆï¼")
print("â¡ å·²ç”Ÿæˆæ¨¡å‹ç‰¹å¾é‡è¦æ€§ï¼šmodel_feature_importance.csv")
